import requests
from bs4 import BeautifulSoup
import csv
import time
import re
from datetime import datetime
import os

class JinPingMeiScraper:
    def __init__(self):
        self.base_url = "https://www.jinpingmei23.tw"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.all_data = []
        
    def get_all_categories(self):
        """è·å–æ‰€æœ‰åˆ†ç±»é“¾æ¥"""
        try:
            response = self.session.get(self.base_url)
            soup = BeautifulSoup(response.content, 'html.parser')
            menu = soup.find('nav', id='menu')
            categories = []
            if menu:
                links = menu.find_all('a')
                for link in links:
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    if href and href != '/' and 'å®šé»' in text or 'å¤–ç´„' in text:
                        full_url = self.base_url + href if href.startswith('/') else href
                        categories.append({'name': text, 'url': full_url})
            return categories
        except Exception as e:
            print(f"è·å–åˆ†ç±»å¤±è´¥: {e}")
            return []
    
    def extract_role_info(self, soup, category_name):
        """ä»é¡µé¢æå–æ‰€æœ‰è§’è‰²ä¿¡æ¯"""
        roles = []
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å—
        images = soup.find_all('img')
        text_content = soup.get_text()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è§’è‰²ä¿¡æ¯æ¨¡å¼
        # åŒ¹é…åå­—å’ŒåŸºæœ¬ä¿¡æ¯ (èº«é«˜.ç½©æ¯.å¹´é¾„)
        pattern = r'([\u4e00-\u9fa5]{2,4})\s*[\n\s]*(\d{3})\.(\d{2})\.(\w)\.(\d{2})Y?'
        matches = re.finditer(pattern, text_content)
        
        for match in matches:
            name = match.group(1)
            height = match.group(2)
            weight = match.group(3)
            cup = match.group(4)
            age = match.group(5)
            
            # æå–è¯¥è§’è‰²åé¢çš„æœåŠ¡ä¿¡æ¯
            start_pos = match.end()
            next_match_pos = text_content.find('ğŸ’°', start_pos)
            if next_match_pos == -1:
                next_match_pos = start_pos + 500
            
            service_text = text_content[start_pos:next_match_pos]
            
            # æå–æœåŠ¡é¡¹ç›®
            service_line = ''
            for line in service_text.split('\n'):
                if 'æœå‹™' in line or 'èˆŒå»' in line or 'æŒ‰æ‘©' in line:
                    service_line = line.strip()
                    break
            
            # æå–ä»·æ ¼
            prices = re.findall(r'ğŸ’°?\s*(\d+)åˆ†.*?(\d{4})', text_content[start_pos:next_match_pos+200])
            price_40 = prices[0][1] if len(prices) > 0 else ''
            price_60 = prices[1][1] if len(prices) > 1 else ''
            
            role = {
                'åˆ†ç±»': category_name,
                'å§“å': name,
                'èº«é«˜': height,
                'ä½“é‡': weight,
                'ç½©æ¯': cup,
                'å¹´é¾„': age,
                'æœåŠ¡é¡¹ç›®': service_line[:100],
                '40åˆ†é’Ÿä»·æ ¼': price_40,
                '60åˆ†é’Ÿä»·æ ¼': price_60,
                'æŠ“å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            roles.append(role)
        
        return roles
    
    def scrape_category(self, category):
        """çˆ¬å–å•ä¸ªåˆ†ç±»çš„æ‰€æœ‰æ•°æ®"""
        try:
            print(f"æ­£åœ¨çˆ¬å–: {category['name']}")
            response = self.session.get(category['url'])
            soup = BeautifulSoup(response.content, 'html.parser')
            
            roles = self.extract_role_info(soup, category['name'])
            self.all_data.extend(roles)
            print(f"  æ‰¾åˆ° {len(roles)} ä¸ªè§’è‰²")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"çˆ¬å– {category['name']} å¤±è´¥: {e}")
    
    def save_to_csv(self, filename='scraped_data.csv'):
        """ä¿å­˜æ•°æ®åˆ°CSV"""
        if not self.all_data:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['åˆ†ç±»', 'å§“å', 'èº«é«˜', 'ä½“é‡', 'ç½©æ¯', 'å¹´é¾„', 'æœåŠ¡é¡¹ç›®', '40åˆ†é’Ÿä»·æ ¼', '60åˆ†é’Ÿä»·æ ¼', 'æŠ“å–æ—¶é—´']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.all_data)
        
        print(f"\næ•°æ®å·²ä¿å­˜åˆ° {filename}")
        print(f"æ€»å…±çˆ¬å– {len(self.all_data)} æ¡è®°å½•")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çˆ¬è™«"""
        print("å¼€å§‹çˆ¬å–é‡‘ç“¶æ¢…ç½‘ç«™...")
        print("="*50)
        
        # è·å–æ‰€æœ‰åˆ†ç±»
        categories = self.get_all_categories()
        print(f"æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»\n")
        
        # çˆ¬å–æ¯ä¸ªåˆ†ç±»
        for i, category in enumerate(categories, 1):
            print(f"[{i}/{len(categories)}] ", end='')
            self.scrape_category(category)
        
        # ä¿å­˜æ•°æ®
        self.save_to_csv()
        print("\nçˆ¬å–å®Œæˆï¼")

if __name__ == '__main__':
    scraper = JinPingMeiScraper()
    scraper.run()
